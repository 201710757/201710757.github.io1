var tipuesearch = {"pages": [{
    "title": "[paper] C51 review",
    "text": "C51 - Categorical RL (Distributional RL) [C51] Distributional RL 에선 기존 Bellman Equation 의 Q-value를 사용하던 것 대신, distribution 기반인 Distributional Bellman Equation을 사용한다. 식은 다음과 같다 $Z(x, a) \\overset{\\underset{\\mathrm{def}}{}}{=} !\\, R(x, a) + γZ(X’, A’)$ 우리가 익히 알고 있던 $Q(x, a) = R(x, a) + γQ(X’, A’)$ 과 비교해보면 단순 $Q$에서 $Z$로 바뀐것밖에 없다. 하지만 여기에 큰 의미가 있는데, $ Q(x_t, a) := \\sum_{i}^{} z_ip_i(x_t, a) $ 위 식을 통해 Z distribution의 기대값을 구하여 기존 Q-value와 같이, action 마다 하나의 scalar값(Q-value)을 구하지만 Distribution을 통해 정확한 분포를 추정하고 그에 따른 Q-value 를 구할 수 있기 때문에 정확도가 더 높습니다. Distributional RL은 action마다 Distribution을 갖고 있는데, 가로축은 분포를 결정하기 위한 하이퍼파라미터로써 support라 불리며, 이 논문에선 51로 하였을 때 결과가 가장 좋았기 때문에 논문 이름도 C51이 되었습니다. distribution의 높이는 각 support의 확률을 뜻합니다. 또한 이 support의 최대, 최소값을 지정해주게 되는데 이 MIN/MAX가 위 그림의 우하단과 같이, 추후 나올 projection에서도 사용됩니다. 다음으로, action마다 distribution이 존재하기 때문에 모델의 아웃풋은 action 수 * support 수 입니다. 결론적으로 이 agent는 각 action의 각 support에 해당하는 확률을 학습하게 됩니다. 이 support에 대한 확률을 어떻게 학습할 것인가가 중요한 문제인데 논문에서는 Wasserstein metric을 사용하려 했지만 그 당시 수렴성을 증명하지 못하였습니다. 따라서 C51논문에선 다음과 같은 Cross Entropy를 통해 Loss를 구하게 됩니다. $ Loss = -\\sum_{i}^{} m_i\\log p_i(x_t, a_t) $ 추후 발표되는 QR-DQN 에서 Wasserstein metric의 수렴성을 증명하게 됩니다. 학습을 시키기 위해 Target Distribution이 필요한데 이논문에서 핵심인 부분이 바로 이부분 입니다. 논문 수식을 차례로 불러오면, $ \\hat{T}z_j ← [r_t + γ_tz_j]^{V_{MAX}}{V{MIN}} $ 우선 오른쪽의 $r_t$는 리워드, $γ_t$는 gamma, ${V_{MAX}}$과 ${V_{MIN}}$는 바로 알 수 있듯 support의 최대, 최소값입니다. 위 식은 기존 DQN에서 target network 구할때와 비슷해보입니다. 실제로도 다 똑같지만 ${V_{MAX}}$과 ${V_{MIN}}$으로 인해 support가 최대값보다 크다면 ${V_{MAX}}$로 맞춰주고, support가 최소값보다 작다면 ${V_{MIN}}$로 맞춰주게 됩니다. $γ_tz_j$를 통해 shirink(보라)되고, $r_t$를 더함(초록)으로써 강제로 맞춰주려하면 아래 그림과 같이 support의 범위가 맞지 않게 됩니다. 이렇게 되면 같은 Domain에 대해 Cross Entropy Loss를 구해야 하는데 그렇지 않기 때문에 Projection이란 과정을 추가합니다. Projection이란 실제 Support가 [1,2,3,4,5]와 같고, Target Distribution의 support가 [1.3, 2.3, 3.5, 4.7, 4.9]이 되었을 때, target Distribution의 support가 1.3인 support의 확률값을 main network의 support 1과 2에 해당하는 곳에 각각 분배하게 됩니다. 나누는 기준은 1.3은 1에 가깝기 때문에 더 많이 배분되고 2에는 멀기 때문에 더 적게 배분됩니다. 해당하는 sudo 코드는 다음과 같습니다. $ l ← \\lfloor{b_j}\\rfloor, u ← \\lceil{b_j}\\rceil $ 위 수식을 통해 해당 target distribution의 support의 low point와 upper point를 구합니다. 예를들면 $b_j$가 3.1일 때, $l$은 3이 되고 $u$는 4가 됩니다. 이를 통해 projection의 핵심기능인 쪼개주기가 가능합니다. $ m_l ← m_l + p_j (x_{t+1}, a^∗)(u − b_j) $ $ m_u ← m_u + p_j (x_{t+1}, a^∗)(b_j − l) $ 위 수식을 통해 main support의 거리에 맞게 target support의 값을 알맞는 비율로 쪼개서 나누어주게 됩니다. 결국 다음 그림과 같아집니다. support도 맞춰주었기 때문에 같은 Domain에서 Loss를 구할 수 있게 되었습니다. 최종적으로 target network의 모든 Support에 대하여 projection까지 마친 뒤 Cross Entropy Loss를 구하여 Update가 가능합니다. 감사합니다",
    "tags": "paper_rl paper",
    "url": "/paper/2021/09/01/20-43-25/"
  },{
    "title": "[devs] github ssh 접속",
    "text": "깃허브에서 clone을 할 때 https 뿐만 아니라 ssh를 통해 가능한 경우도 있습니다. https는 요즘 8월 에러가 뜨길래 귀찮기도 하고 ssh가 편하기도 해서 포스트 해보았습니다! 먼저 터미널을 연 뒤, ssh-keygen -t ed25519 -C “githubEmail@email.com” 또는 ssh-keygen -t rsa -b 4096 -C “githubEmail@email.com” 를 입력후 정상적으로 동작하면, .ssh폴더에 들어가 id.pub 파일의 내용을 복사합니다 -&gt; [Windows] clip &lt; id.pub -&gt; [MAC] pbcopy &lt; id.pub 다음으로 깃허브 홈페이지에 들어가 오른쪽 상단의 프로필 클릭후, 세팅에 들어간 뒤 왼쪽메뉴에 ssh and gpg keys 를 들어가면 오른쪽 위에 New SSH key버튼이 있습니다. 눌러 들어간 뒤, 타이틀은 원하는대로 작성하시고 key 부분에 붙여넣기 해주시면 등록은 끝입니다. 접속설정을 추가로 해주어야 하는데 우선 같은 .ssh 폴더 안에 config파일을 열어준 뒤 Host github.com IdentityFile 경로/.ssh/id.pub User git 를 맨 밑에 추가해주시면 정상적으로 클론해올 수 있습니다! 감사합니다!",
    "tags": "devs_else devs",
    "url": "/devs/2021/08/24/01-27-24/"
  },{
    "title": "[devs] Logstash 위경도 좌표변환 for Kibana Map",
    "text": "보통 우리가 가지고 있는 String형 위경도 좌표를 키바나 맵에 뿌리기 위해선 바로 사용가능 한것이 아니라 geo-point type으로 바꿔줘야 kibana map이 읽어올 수 있는데 이 과정을 한번 정리해보겠습니다. 우선 현재 text타입인 위경도 좌표를 알맞게 변환해 넣을 공간인 geo-point를 dev tools 에서 다음과 같은 코드를 통해 만들어주어야 합니다. 다음으로, 위 과정에서 만든 인덱스에 데이터를 넣기위해 로그스태시에서 사용할 conf file을 확인해보겠습니다. 사실 중요한건 아래 코드입니다. 123456mutate&nbsp;{&nbsp;convert&nbsp;=&gt;&nbsp;{'lat'&nbsp;=&gt;&nbsp;'float'}&nbsp;}mutate&nbsp;{&nbsp;convert&nbsp;=&gt;&nbsp;{'lon'&nbsp;=&gt;&nbsp;'float'}&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mutate&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;add_field&nbsp;=&gt;&nbsp;{'[location-geo][lat]'&nbsp;=&gt;&nbsp;'%{lat}'}&nbsp;&nbsp;&nbsp;&nbsp;add_field&nbsp;=&gt;&nbsp;{'[location-geo][lon]'&nbsp;=&gt;&nbsp;'%{lon}'}}Colored by Color Scriptercs 원본소스의 중반부분을 가져왔습니다. 확인해보면 1번, 2번 라인은 기존 text(String)형이었던 lat과 lon을 geo-point type이 읽을 수 있는 float형으로 변환해주고 있습니다. 다음으로 3번부터 6번 라인은 앞서 컬럼을 만들어두었던 곳에 대입하는 코드입니다. 이로써 미리 만들어 두었던 location-geo 컬럼의 geo-point type에 lat 과 lon이 대입됩니다. 이를통해 기존 텍스트 값이던 위경도 좌표를 키바나 맵에서 읽고 뿌릴 수 있게 되었습니다! 결과적으로 위 사진과 같이 잘 나오는 것을 확인할 수 있습니다! 감사합니다.",
    "tags": "devs_else devs",
    "url": "/devs/2021/08/19/01-25-44/"
  },{
    "title": "[devs] scp 우분투 파일전송",
    "text": "command를 통해 우분투로 파일 또는 폴더를 보내고 받기를 해봅시다! local to remote (보내기) scp -P 1234 myfile user@192.168.0.10:/home/location 서버의 주소는 192.168.0.10, 포트는 1234일 때 user란 아이디로 connect하고 /home/location 에 myfile을 보내고싶을때 (파일 여러개 보내고 싶은경우는 myfile myfile1처럼 이어 붙이기만 하면 됩니다.) (폴더를 보내고 싶을 땐 파일명 위치에 -r 옵션과 함께 폴더명을 적으면 됩니다!) remote to local (받기) scp -P 1234 user@192.168.0.10:/home/location/file1 /loc/ 192.168.0.10:1234 의 서버에 user란 아이디로 연결하고, 서버의 /home/location/ 경로에 위치하고 있는 file1파일을 내컴퓨터의 /loc/ 경로에 받아오고 싶을 때 (보내기와 마찬가지로 여러파일 받아오고 싶을때, 파일경로를 띄어쓰기로 구분하여 모두 입력하고 쌍따옴표(“)로 묶고, 다음위치에 저장하고싶은 경로를 작성하면 됩니다.) (폴더를 받고 싶을 때, -r옵션을 user앞에 넣어준뒤 파일명 대신 폴더명을 적으면 됩니다!)",
    "tags": "devs_else devs",
    "url": "/devs/2021/08/18/01-24-26/"
  },{
    "title": "[devs] 스크린샷 업무 자동화",
    "text": "몇년전 공익근무 관련 업무자동화로 이슈가 있었던 토픽입니다! 엄청난 스크린샷 업무를 파이썬 하나를 통해 몇달걸일 일을 단숨에 끝냈다고 하네요. 마침 스크린샷 후 pdf파일 만들 일이 생겨, 상기 시나리오를 기반으로 구현을 해보려고 합니다. 우선 전체코드는 다음과 같습니다. 123456789import&nbsp;pyautogui&nbsp;page&nbsp;=&nbsp;250for&nbsp;i&nbsp;in&nbsp;range(page):&nbsp;&nbsp;&nbsp;&nbsp;try:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.screenshot('filename'+str(i)+'.png',&nbsp;region=(200,300,&nbsp;300,300))&nbsp;&nbsp;&nbsp;&nbsp;except:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.click(900,2000,&nbsp;button='left',&nbsp;clicks=1,&nbsp;interval=1)cs 1import&nbsp;pyautoguics 먼저, 우리가 사용할 라이브러리는 pyautogui 입니다. 이 라이브러리를 통해 스크린샷과 원하는 위치를 클릭하는 기능을 구현할 것입니다. 1page&nbsp;=&nbsp;250cs 페이지는 원하시는만큼 할당해 주시고, 12345for&nbsp;i&nbsp;in&nbsp;range(page):&nbsp;&nbsp;&nbsp;&nbsp;try:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pyautogui.screenshot('filename'+str(i)+'.png',&nbsp;region=(200,300,&nbsp;300,300))&nbsp;&nbsp;&nbsp;&nbsp;except:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;passColored by Color Scriptercs 반복문을 돌며 pyautogui.screenshot 함수를 통해 스크린샷을 찍게 되는데 먼저 저장 파일명이 인자로 들어가고, 위치를 지정해줘야 하는데 region에 차례로 시작할 x좌표, y좌표, x크기, y크기 가 들어가게 됩니다. try 문으로 감싸주는것은 대용량 작업시 뻑날 경우를 대비해서 추가해보았습니다 ㅋㅋ 1pyautogui.click(900,2000,&nbsp;button='left',&nbsp;clicks=1,&nbsp;interval=1)cs 마지막으로, 클릭해서 페이지를 넘기거나 정보를 수정해야 하기 때문에, pyautogui.click함수를 활용하여 클릭을 하게 됩니다. 인자를 차례로 보면, 클릭할 x좌표, y좌표, clicks에서 볼 수 있듯 몇번 클릭할지, interval은 클릭 사이 간격을 몇초로 둘것인가를 설정하는 것입니다. 이 interval로 인해 따로 sleep함수를 걸어주지 않아도 알아서 중간중간 실행을 기다릴 수 있습니다. 이로써 구현 완료입니다! 입맛대로 수정해보면서 라이브러리를 익히는 것도 좋겠네요! 감사합니다~",
    "tags": "devs_else devs",
    "url": "/devs/2021/07/13/01-21-46/"
  },{
    "title": "[paper] 논문 공부 리스트",
    "text": "Last Update : 2021-08-18 읽고 공부할 논문들 GAN [pytorch-GANs] RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion [RL-GAN] RL DQN-2013 : Playing Atari with Deep Reinforcement Learning [DQN-2013] DQN-2015 : Human-level control through deep reinforcement learning [DQN-2015] Double-DQN : Deep Reinforcement Learning with Double Q-learning [Double-DQN] Dueling-DQN : Dueling Network Architectures for Deep Reinforcement Learning [Dueling-DQN] C51 : A Distributional Perspective on Reinforcement Learning [C51] QR-DQN : Distributional Reinforcement Learning with Quantile Regression [QR-DQN] IQN : Implicit Quantile Networks for Distributional Reinforcement Learning [IQN] MARL COMA : Counterfactual Multi-Agent Policy Gradients [COMA] QMIX : QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning [QMIX] MAAC : Actor-Attention-Critic for Multi-Agent Reinforcement Learning [MAAC] ETC Noisy Network : Noisy Networks for Exploration [Noisy-Net]",
    "tags": "paper_else paper",
    "url": "/paper/2021/07/08/01-19-01/"
  }]};
